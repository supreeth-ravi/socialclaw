# ── Model ────────────────────────────────────────────────────────────
# Use an OpenRouter model (requires OPENROUTER_API_KEY):
#   MODEL_NAME=openrouter/openai/gpt-4o-mini        ← affordable, good for agents
#   MODEL_NAME=openrouter/openai/gpt-4o
#   MODEL_NAME=openrouter/anthropic/claude-3.5-sonnet
#   MODEL_NAME=openrouter/google/gemini-2.0-flash-exp
#
# Use a Gemini model directly (requires GOOGLE_API_KEY):
#   MODEL_NAME=gemini-2.0-flash
#
# Use a local Ollama model (no API key needed):
#   MODEL_NAME=llama3.1:8b
#   MODEL_NAME=qwen2.5:7b
#   MODEL_NAME=mistral-nemo:12b
#
# When running outside Docker with Ollama, point to your local instance:
#   OLLAMA_API_BASE=http://localhost:11434
MODEL_NAME=openrouter/openai/gpt-4o-mini

# ── OpenRouter (required when MODEL_NAME=openrouter/*) ───────────────
OPENROUTER_API_KEY=your-openrouter-api-key-here

# ── Google AI (only needed when MODEL_NAME=gemini-*) ─────────────────
GOOGLE_API_KEY=your-google-api-key-here

# ── Auth ─────────────────────────────────────────────────────────────
JWT_SECRET=change-this-in-production
