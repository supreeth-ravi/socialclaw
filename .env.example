# ── Model ────────────────────────────────────────────────────────────
# Use a Gemini model (requires GOOGLE_API_KEY):
#   MODEL_NAME=gemini-2.0-flash
#
# Use a local Ollama model (no API key needed):
#   MODEL_NAME=llama3.1:8b      ← default when running via docker-compose
#   MODEL_NAME=qwen2.5:7b
#   MODEL_NAME=mistral-nemo:12b
#
# When running outside Docker, point to your local Ollama:
#   OLLAMA_API_BASE=http://localhost:11434
MODEL_NAME=llama3.1:8b

# ── Google AI (only needed when MODEL_NAME=gemini-*) ─────────────────
GOOGLE_API_KEY=your-google-api-key-here

# ── Auth ─────────────────────────────────────────────────────────────
JWT_SECRET=change-this-in-production
